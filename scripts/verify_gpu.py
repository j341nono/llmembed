#!/usr/bin/env python3
"""
GPU Verification Script for llemb v0.2.2
==========================================

This script verifies that the llemb library works correctly in a GPU environment
with quantization enabled. It tests the new Smart Default API introduced in v0.2.2.

Requirements:
- CUDA-capable GPU
- bitsandbytes library (now a core dependency in v0.2.2)
- torch with CUDA support
"""

import sys
from typing import Optional


def print_header(title: str) -> None:
    """Print a formatted header."""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def print_success(message: str) -> None:
    """Print a success message."""
    print(f"âœ“ {message}")


def print_error(message: str) -> None:
    """Print an error message."""
    print(f"âœ— {message}", file=sys.stderr)


def print_info(message: str) -> None:
    """Print an info message."""
    print(f"â„¹ {message}")


def check_cuda() -> bool:
    """
    Check if CUDA is available.
    
    Returns:
        True if CUDA is available, False otherwise.
    """
    print_header("Step 1: CUDA Availability Check")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            cuda_version = torch.version.cuda
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            
            print_success(f"CUDA is available (version: {cuda_version})")
            print_info(f"Detected {device_count} CUDA device(s)")
            print_info(f"Primary device: {device_name}")
            return True
        else:
            print_error("CUDA is NOT available")
            print_info("This script requires a CUDA-capable GPU")
            print_info("Please ensure:")
            print_info("  1. You have a CUDA-capable GPU")
            print_info("  2. NVIDIA drivers are installed")
            print_info("  3. PyTorch is installed with CUDA support")
            return False
            
    except ImportError as e:
        print_error(f"Failed to import torch: {e}")
        return False


def check_bitsandbytes() -> bool:
    """
    Check if bitsandbytes is installed and can link to CUDA.
    
    Returns:
        True if bitsandbytes is properly installed, False otherwise.
    """
    print_header("Step 2: bitsandbytes Dependency Check")
    
    try:
        import bitsandbytes as bnb
        
        print_success("bitsandbytes is installed")
        
        # Try to get CUDA setup info
        try:
            bnb_version = bnb.__version__
            print_info(f"bitsandbytes version: {bnb_version}")
        except AttributeError:
            print_info("bitsandbytes version: unknown")
        
        # Verify CUDA linking
        try:
            import torch
            if torch.cuda.is_available():
                # Try to create a simple quantized operation to verify CUDA linking
                test_tensor = torch.randn(10, 10).cuda()
                print_success("bitsandbytes can successfully link to CUDA")
            else:
                print_info("Skipping CUDA linking test (CUDA not available)")
        except Exception as e:
            print_error(f"bitsandbytes CUDA linking test failed: {e}")
            return False
        
        return True
        
    except ImportError as e:
        print_error(f"bitsandbytes is NOT installed: {e}")
        print_info("As of v0.2.2, bitsandbytes is a core dependency")
        print_info("It should have been installed automatically with llemb")
        print_info("If not, try: pip install bitsandbytes>=0.48.2")
        return False


def test_model_loading(
    model_name: str = "HuggingFaceTB/SmolLM2-135M",
    quantization: str = "4bit"
) -> Optional[object]:
    """
    Test loading a model with quantization enabled.
    
    Args:
        model_name: HuggingFace model identifier (using a small model for quick testing)
        quantization: Quantization mode ('4bit' or '8bit')
    
    Returns:
        Encoder instance if successful, None otherwise.
    """
    print_header(f"Step 3: Model Loading with {quantization.upper()} Quantization")
    
    try:
        import llemb
        
        print_info(f"Loading model: {model_name}")
        print_info(f"Quantization: {quantization}")
        print_info("This may take a moment...")
        
        # Initialize encoder with quantization
        # Note: device_map="auto" is set automatically when using quantization
        enc = llemb.Encoder(
            model_name=model_name,
            backend="transformers",
            device="cuda",
            quantization=quantization
        )
        
        print_success(f"Model loaded successfully with {quantization} quantization")
        
        # Get model device info
        model_device = next(enc.backend_instance.model.parameters()).device
        print_info(f"Model is on device: {model_device}")
        
        # Check if model is quantized
        if hasattr(enc.backend_instance.model, "hf_device_map"):
            print_info("Model is using device_map (distributed across devices)")
        
        return enc
        
    except ImportError as e:
        print_error(f"Failed to import llemb: {e}")
        print_info("Make sure llemb is installed: pip install -e .")
        return None
    except Exception as e:
        print_error(f"Failed to load model: {e}")
        print_info("This could be due to:")
        print_info("  1. Insufficient GPU memory")
        print_info("  2. Model not found on HuggingFace Hub")
        print_info("  3. bitsandbytes compatibility issues")
        return None


def test_inference_smart_defaults(enc: object) -> bool:
    """
    Test inference using the new Smart Default API (v0.2.2).
    
    Args:
        enc: Encoder instance
    
    Returns:
        True if inference succeeds, False otherwise.
    """
    print_header("Step 4: Inference Test with Smart Defaults API")
    
    try:
        import torch
        
        test_texts = [
            "GPU Test",
            "Testing quantized inference with Smart Defaults",
            "The new API automatically uses last_token pooling with templates"
        ]
        
        print_info("Testing new Smart Default API (v0.2.2)")
        print_info("Feature: pooling_method auto-defaults to 'last_token' with prompt_template")
        print()
        
        # Test 1: Smart Default with pcoteol template
        print_info("Test 1: Using prompt_template='pcoteol' (should auto-use last_token)")
        embeddings_1 = enc.encode(test_texts[0], prompt_template="pcoteol")
        
        if isinstance(embeddings_1, torch.Tensor):
            print_success(f"Embedding generated: shape={embeddings_1.shape}, dtype={embeddings_1.dtype}")
        else:
            print_error(f"Unexpected output type: {type(embeddings_1)}")
            return False
        
        # Test 2: Smart Default with ke template
        print()
        print_info("Test 2: Using prompt_template='ke' (should auto-use last_token)")
        embeddings_2 = enc.encode(test_texts[1], prompt_template="ke")
        print_success(f"Embedding generated: shape={embeddings_2.shape}, dtype={embeddings_2.dtype}")
        
        # Test 3: Batch processing
        print()
        print_info("Test 3: Batch processing with Smart Defaults")
        embeddings_batch = enc.encode(test_texts, prompt_template="prompteol")
        print_success(f"Batch embeddings: shape={embeddings_batch.shape}, dtype={embeddings_batch.dtype}")
        
        # Verify embeddings are valid (no NaN or Inf)
        if torch.isnan(embeddings_batch).any():
            print_error("Embeddings contain NaN values")
            return False
        
        if torch.isinf(embeddings_batch).any():
            print_error("Embeddings contain Inf values")
            return False
        
        print_success("All embeddings are valid (no NaN or Inf)")
        
        # Test 4: Explicit pooling_method (override Smart Default)
        print()
        print_info("Test 4: Explicit pooling_method='mean' (overrides Smart Default)")
        embeddings_mean = enc.encode(
            test_texts[0],
            pooling_method="mean",
            prompt_template="pcoteol"
        )
        print_success(f"Embedding with explicit mean: shape={embeddings_mean.shape}")
        
        # Verify that explicit override produces different result
        embeddings_default = enc.encode(test_texts[0], prompt_template="pcoteol")
        if not torch.allclose(embeddings_mean, embeddings_default):
            print_success("Explicit override correctly produces different embeddings")
        else:
            print_info("Note: Embeddings are similar (may vary by model)")
        
        return True
        
    except Exception as e:
        print_error(f"Inference test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_layer_defaults(enc: object) -> bool:
    """
    Test layer index defaults (pcoteol/ke use layer -2 by default).
    
    Args:
        enc: Encoder instance
    
    Returns:
        True if test succeeds, False otherwise.
    """
    print_header("Step 5: Layer Index Defaults Test")
    
    try:
        import torch
        
        print_info("Testing Smart Layer Defaults:")
        print_info("  - 'pcoteol' and 'ke' templates should use layer -2")
        print_info("  - 'prompteol' and no template should use layer -1")
        
        # Test pcoteol default (layer -2)
        emb_pcoteol_default = enc.encode(
            "Test",
            prompt_template="pcoteol",
            layer_index=None  # Should default to -2
        )
        
        emb_pcoteol_explicit = enc.encode(
            "Test",
            prompt_template="pcoteol",
            layer_index=-2
        )
        
        if torch.allclose(emb_pcoteol_default, emb_pcoteol_explicit, atol=1e-6):
            print_success("pcoteol correctly defaults to layer -2")
        else:
            print_error("pcoteol layer default mismatch")
            return False
        
        # Test mean pooling default (layer -1)
        emb_mean_default = enc.encode(
            "Test",
            pooling_method="mean",
            layer_index=None  # Should default to -1
        )
        
        emb_mean_explicit = enc.encode(
            "Test",
            pooling_method="mean",
            layer_index=-1
        )
        
        if torch.allclose(emb_mean_default, emb_mean_explicit, atol=1e-6):
            print_success("mean pooling correctly defaults to layer -1")
        else:
            print_error("mean pooling layer default mismatch")
            return False
        
        return True
        
    except Exception as e:
        print_error(f"Layer defaults test failed: {e}")
        return False


def main() -> int:
    """
    Main verification routine.
    
    Returns:
        0 if all checks pass, 1 otherwise.
    """
    print("\n" + "=" * 80)
    print("  llemb GPU Verification Script (v0.2.2)")
    print("  Testing: Quantization + Smart Defaults API")
    print("=" * 80)
    
    # Step 1: Check CUDA
    if not check_cuda():
        print_error("\nVerification FAILED: CUDA is not available")
        return 1
    
    # Step 2: Check bitsandbytes
    if not check_bitsandbytes():
        print_error("\nVerification FAILED: bitsandbytes is not properly installed")
        return 1
    
    # Step 3: Load model with quantization
    enc = test_model_loading()
    if enc is None:
        print_error("\nVerification FAILED: Could not load model with quantization")
        return 1
    
    # Step 4: Test inference with Smart Defaults
    if not test_inference_smart_defaults(enc):
        print_error("\nVerification FAILED: Inference test failed")
        return 1
    
    # Step 5: Test layer defaults
    if not test_layer_defaults(enc):
        print_error("\nVerification FAILED: Layer defaults test failed")
        return 1
    
    # All checks passed!
    print_header("ðŸŽ‰ VERIFICATION SUCCESS ðŸŽ‰")
    print()
    print("All checks passed successfully!")
    print()
    print("Summary:")
    print("  âœ“ CUDA is available and working")
    print("  âœ“ bitsandbytes is properly installed")
    print("  âœ“ Model loading with quantization works")
    print("  âœ“ Smart Default API (v0.2.2) works correctly")
    print("  âœ“ Inference produces valid embeddings")
    print("  âœ“ Layer index defaults work as expected")
    print()
    print("Your GPU environment is ready to use llemb v0.2.2!")
    print("=" * 80)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
